import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_curve
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("=" * 80)
print("   ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ & ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸")
print("   ì—…ê³„ í‘œì¤€ ì ‘ê·¼ë²• - ì¼ë°˜ ì˜¨ë¼ì¸ ë¦¬í…Œì¼")
print("=" * 80)

# ========================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
# ========================================================================
print("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„")
print("-" * 50)

# RFM ë¶„ì„ ê²°ê³¼ ë¡œë“œ
rfm = pd.read_csv('results/rfm_result/rfm_analysis_results.csv')
print(f"RFM ë°ì´í„° ë¡œë“œ: {rfm.shape}")

# ì›ë³¸ ê±°ë˜ ë°ì´í„° ë¡œë“œ (ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±ìš©)
df = pd.read_csv('data/processed/cleaned_retail_data.csv', encoding='ISO-8859-1')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df['InvoiceNo'] = df['InvoiceNo'].astype(str)
# ì •ìƒ ê±°ë˜ë§Œ í•„í„°ë§
df_clean = df[
    (df['Quantity'] > 0) & 
    (df['UnitPrice'] > 0) & 
    (~df['InvoiceNo'].str.contains('C', na=False)) &
    (df['CustomerID'].notna())
].copy()

df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['UnitPrice']
print(f"ì •ìƒ ê±°ë˜ ë°ì´í„°: {df_clean.shape}")

# ë¶„ì„ ê¸°ì¤€ì¼
analysis_date = df_clean['InvoiceDate'].max() + timedelta(days=1)
print(f"ë¶„ì„ ê¸°ì¤€ì¼: {analysis_date}")

# ========================================================================
# 2. ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ========================================================================
print("\nâ° 2ë‹¨ê³„: ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
print("-" * 50)

def extract_time_series_features(customer_id, transactions_df, analysis_date):
    """
    ë„ë©”ì¸ ì§€ì‹: êµ¬ë§¤ íŒ¨í„´ì˜ ì‹œê°„ì  ë³€í™”ë¥¼ í¬ì°©í•˜ëŠ” ì¼ë°˜ì ì¸ í”¼ì²˜ë“¤
    - êµ¬ë§¤ ê°„ê²©ì˜ ë³€í™”
    - êµ¬ë§¤ ê¸ˆì•¡ì˜ ë³€í™”
    - êµ¬ë§¤ ë¹ˆë„ì˜ ë³€í™”
    - ìµœê·¼ í™œë™ ìˆ˜ì¤€
    """
    customer_trans = transactions_df[transactions_df['CustomerID'] == customer_id].sort_values('InvoiceDate')
    
    features = {}
    
    # ê¸°ë³¸ ì •ë³´
    features['total_transactions'] = len(customer_trans)
    features['unique_purchase_days'] = customer_trans['InvoiceDate'].dt.date.nunique()
    
    if len(customer_trans) < 2:
        # ë‹¨ì¼ êµ¬ë§¤ ê³ ê°ì˜ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        features['avg_days_between_purchases'] = np.nan
        features['std_days_between_purchases'] = np.nan
        features['purchase_interval_trend'] = 0
        features['purchase_amount_trend'] = 0
        features['frequency_trend'] = 0
        features['monetary_velocity'] = 0
        features['purchase_regularity'] = 0
        features['recent_activity_ratio_30d'] = 0
        features['recent_activity_ratio_60d'] = 0
        features['recent_activity_ratio_90d'] = 0
        return features
    
    # êµ¬ë§¤ ë‚ ì§œë³„ ì§‘ê³„
    daily_purchases = customer_trans.groupby(customer_trans['InvoiceDate'].dt.date).agg({
        'TotalAmount': 'sum',
        'InvoiceNo': 'nunique'
    }).reset_index()
    daily_purchases.columns = ['Date', 'Amount', 'Orders']
    
    # 1. êµ¬ë§¤ ê°„ê²© ë¶„ì„
    purchase_dates = pd.to_datetime(daily_purchases['Date'])
    intervals = np.diff(purchase_dates).astype('timedelta64[D]').astype(float)
    
    features['avg_days_between_purchases'] = np.mean(intervals) if len(intervals) > 0 else 0
    features['std_days_between_purchases'] = np.std(intervals) if len(intervals) > 0 else 0
    features['min_days_between_purchases'] = np.min(intervals) if len(intervals) > 0 else 0
    features['max_days_between_purchases'] = np.max(intervals) if len(intervals) > 0 else 0
    
    # êµ¬ë§¤ ê°„ê²© íŠ¸ë Œë“œ (ê°„ê²©ì´ ì¦ê°€í•˜ë©´ ì–‘ìˆ˜)
    if len(intervals) > 1:
        features['purchase_interval_trend'] = np.polyfit(range(len(intervals)), intervals, 1)[0]
    else:
        features['purchase_interval_trend'] = 0
    
    # 2. êµ¬ë§¤ ê¸ˆì•¡ ë¶„ì„
    amounts = daily_purchases['Amount'].values
    
    # êµ¬ë§¤ ê¸ˆì•¡ íŠ¸ë Œë“œ
    if len(amounts) > 1:
        features['purchase_amount_trend'] = np.polyfit(range(len(amounts)), amounts, 1)[0]
        
        # ìµœê·¼ vs ê³¼ê±° ê¸ˆì•¡ ë¹„êµ
        mid_point = len(amounts) // 2
        recent_avg_amount = np.mean(amounts[mid_point:])
        past_avg_amount = np.mean(amounts[:mid_point])
        features['amount_trend_ratio'] = (recent_avg_amount / (past_avg_amount + 1)) - 1
    else:
        features['purchase_amount_trend'] = 0
        features['amount_trend_ratio'] = 0
    
    # 3. êµ¬ë§¤ ë¹ˆë„ ë¶„ì„
    # ì‹œê°„ì„ 3ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¹ˆë„ ë³€í™” ì¸¡ì •
    time_span = (customer_trans['InvoiceDate'].max() - customer_trans['InvoiceDate'].min()).days
    if time_span > 0:
        third = time_span / 3
        
        period1 = customer_trans[customer_trans['InvoiceDate'] <= 
                                 customer_trans['InvoiceDate'].min() + timedelta(days=third)]
        period2 = customer_trans[(customer_trans['InvoiceDate'] > 
                                  customer_trans['InvoiceDate'].min() + timedelta(days=third)) &
                                 (customer_trans['InvoiceDate'] <= 
                                  customer_trans['InvoiceDate'].min() + timedelta(days=2*third))]
        period3 = customer_trans[customer_trans['InvoiceDate'] > 
                                 customer_trans['InvoiceDate'].min() + timedelta(days=2*third)]
        
        freq1 = len(period1) / max(third, 1)
        freq2 = len(period2) / max(third, 1)
        freq3 = len(period3) / max(third, 1)
        
        # ë¹ˆë„ íŠ¸ë Œë“œ (ê°ì†Œí•˜ë©´ ìŒìˆ˜)
        if freq1 > 0:
            features['frequency_trend'] = (freq3 - freq1) / freq1
        else:
            features['frequency_trend'] = 0
            
        # ë¹ˆë„ ê°€ì†ë„ (ë³€í™”ìœ¨ì˜ ë³€í™”)
        features['frequency_acceleration'] = (freq3 - freq2) - (freq2 - freq1)
    else:
        features['frequency_trend'] = 0
        features['frequency_acceleration'] = 0
    
    # 4. êµ¬ë§¤ ê·œì¹™ì„± (ë³€ë™ê³„ìˆ˜ì˜ ì—­ìˆ˜)
    if features['avg_days_between_purchases'] > 0:
        cv = features['std_days_between_purchases'] / features['avg_days_between_purchases']
        features['purchase_regularity'] = 1 / (1 + cv)  # 0~1 ì‚¬ì´ ê°’, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê·œì¹™ì 
    else:
        features['purchase_regularity'] = 0
    
    # 5. ê¸ˆì•¡ ì†ë„ (Monetary Velocity)
    days_active = (customer_trans['InvoiceDate'].max() - customer_trans['InvoiceDate'].min()).days + 1
    features['monetary_velocity'] = customer_trans['TotalAmount'].sum() / max(days_active, 1)
    
    # 6. ìµœê·¼ í™œë™ ë¹„ìœ¨ (ë‹¤ì–‘í•œ ê¸°ê°„)
    for days in [30, 60, 90]:
        recent_date = analysis_date - timedelta(days=days)
        recent_trans = customer_trans[customer_trans['InvoiceDate'] >= recent_date]
        features[f'recent_activity_ratio_{days}d'] = len(recent_trans) / len(customer_trans)
        features[f'recent_monetary_ratio_{days}d'] = (recent_trans['TotalAmount'].sum() / 
                                                      customer_trans['TotalAmount'].sum() 
                                                      if len(recent_trans) > 0 else 0)
    
    # 7. êµ¬ë§¤ ì‹œì  íŒ¨í„´
    features['weekend_purchase_ratio'] = customer_trans['InvoiceDate'].dt.dayofweek.isin([5, 6]).mean()
    features['month_diversity'] = customer_trans['InvoiceDate'].dt.month.nunique() / 12
    
    # 8. ì œí’ˆ ë‹¤ì–‘ì„± (ì¹´í…Œê³ ë¦¬ ì—†ì´ ì œí’ˆ ìˆ˜ë¡œ ì¸¡ì •)
    features['unique_products'] = customer_trans['StockCode'].nunique()
    features['avg_products_per_order'] = customer_trans.groupby('InvoiceNo')['StockCode'].nunique().mean()
    
    return features

# ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ (ì „ì²´ ê³ ê°)
print("ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ ì¤‘...")
time_features_list = []

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ê°œì„ 
batch_size = 500
customer_ids = rfm['CustomerID'].values

for i in range(0, len(customer_ids), batch_size):
    batch_ids = customer_ids[i:i+batch_size]
    batch_features = []
    
    for customer_id in batch_ids:
        features = extract_time_series_features(customer_id, df_clean, analysis_date)
        features['CustomerID'] = customer_id
        batch_features.append(features)
    
    time_features_list.extend(batch_features)
    
    if (i + batch_size) % 2000 == 0:
        print(f"  ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, len(customer_ids))}/{len(customer_ids)}")

time_features_df = pd.DataFrame(time_features_list)
print(f"ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {time_features_df.shape}")

# RFM ë°ì´í„°ì™€ ë³‘í•©
data = rfm.merge(time_features_df, on='CustomerID', how='left')
print(f"í†µí•© ë°ì´í„°: {data.shape}")

# ========================================================================
# 3. ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ========================================================================
print("\nğŸ”§ 3ë‹¨ê³„: ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: ì´íƒˆ ì˜ˆì¸¡ì— ìœ ìš©í•œ íŒŒìƒ í”¼ì²˜ë“¤

# 1. RFM ìƒí˜¸ì‘ìš© í”¼ì²˜
data['RF_Score'] = data['R_Score'] * data['F_Score']
data['RM_Score'] = data['R_Score'] * data['M_Score']
data['FM_Score'] = data['F_Score'] * data['M_Score']
data['RFM_Score_Sum'] = data['R_Score'] + data['F_Score'] + data['M_Score']

# 2. ê³ ê° ìƒì•  ê°€ì¹˜ ê´€ë ¨
data['customer_lifetime_days'] = data.apply(
    lambda x: (analysis_date - df_clean[df_clean['CustomerID'] == x['CustomerID']]['InvoiceDate'].min()).days if pd.notna(x['CustomerID']) else 0,
    axis=1
)
data['lifetime_value_per_day'] = data['Monetary'] / (data['customer_lifetime_days'] + 1)

# 3. êµ¬ë§¤ íš¨ìœ¨ì„±
data['purchase_efficiency'] = data['Monetary'] / (data['Frequency'] * data['avg_days_between_purchases'] + 1)

# 4. ì´íƒˆ ìœ„í—˜ ì§€í‘œ (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜)
# êµ¬ë§¤ ê°„ê²©ì´ í‰ê· ë³´ë‹¤ 2ë°° ì´ìƒ ì¦ê°€
data['interval_risk'] = (data['purchase_interval_trend'] > data['avg_days_between_purchases']).astype(int)

# ìµœê·¼ 30ì¼ í™œë™ì´ ì „ì²´ ê¸°ê°„ ëŒ€ë¹„ 10% ë¯¸ë§Œ
data['activity_risk'] = (data['recent_activity_ratio_30d'] < 0.1).astype(int)

# ê¸ˆì•¡ íŠ¸ë Œë“œê°€ -30% ì´í•˜
data['monetary_risk'] = (data['amount_trend_ratio'] < -0.3).astype(int)

# ì¢…í•© ìœ„í—˜ ì ìˆ˜
data['risk_score'] = data['interval_risk'] + data['activity_risk'] + data['monetary_risk']

# 5. ì„¸ê·¸ë¨¼íŠ¸ ì¸ì½”ë”©
segment_mapping = {
    'Champions': 11,
    'Loyal Customers': 10,
    'Potential Loyalists': 9,
    'New Customers': 8,
    'Promising': 7,
    'Need Attention': 6,
    'About to Sleep': 5,
    'At Risk': 4,
    'Cannot Lose Them': 3,
    'Hibernating': 2,
    'Lost': 1,
    'Others': 0
}
data['Segment_Encoded'] = data['Segment'].map(segment_mapping).fillna(0)

print("ì¶”ê°€ í”¼ì²˜ ìƒì„± ì™„ë£Œ")
print(f"ì´ í”¼ì²˜ ìˆ˜: {len(data.columns)}")

# ========================================================================
# 4. ì´íƒˆ ë¼ë²¨ ì •ì˜ (120ì¼ ê¸°ì¤€)
# ========================================================================
print("\nğŸ¯ 4ë‹¨ê³„: ì´íƒˆ ë¼ë²¨ ì •ì˜ (120ì¼ ê¸°ì¤€)")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: ì¼ë°˜ ì˜¨ë¼ì¸ ë¦¬í…Œì¼ 120ì¼ ì´íƒˆ ê¸°ì¤€
CHURN_THRESHOLD = 120
data['is_churned'] = (data['Recency'] > CHURN_THRESHOLD).astype(int)

print(f"ì´íƒˆ ê¸°ì¤€: {CHURN_THRESHOLD}ì¼")
print(f"ì „ì²´ ì´íƒˆë¥ : {data['is_churned'].mean():.1%}")
print(f"- í™œì„± ê³ ê°: {(data['is_churned'] == 0).sum():,}ëª…")
print(f"- ì´íƒˆ ê³ ê°: {(data['is_churned'] == 1).sum():,}ëª…")

# í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
print(f"\ní´ë˜ìŠ¤ ë¹„ìœ¨: 1:{(data['is_churned'] == 0).sum() / (data['is_churned'] == 1).sum():.1f}")

# ========================================================================
# 5. í”¼ì²˜ ì„ íƒ ë° ì „ì²˜ë¦¬
# ========================================================================
print("\nğŸ“ 5ë‹¨ê³„: í”¼ì²˜ ì„ íƒ ë° ì „ì²˜ë¦¬")
print("-" * 50)

# ì˜ˆì¸¡ì— ì‚¬ìš©í•  í”¼ì²˜ ì„ íƒ (Recency ì œì™¸ - íƒ€ê²Ÿ ë¦¬í‚¤ì§€ ë°©ì§€)
feature_cols = [
    # RFM ê¸°ë³¸ (Recency ì œì™¸)
    'Frequency', 'Monetary',
    'F_Score', 'M_Score',
    
    # RFM íŒŒìƒ
    'FM_Score', 'RFM_Score_Sum',
    'avg_order_value',
    
    # ì‹œê³„ì—´ í”¼ì²˜
    'total_transactions', 'unique_purchase_days',
    'avg_days_between_purchases', 'std_days_between_purchases',
    'min_days_between_purchases', 'max_days_between_purchases',
    'purchase_interval_trend', 'purchase_amount_trend',
    'amount_trend_ratio', 'frequency_trend', 'frequency_acceleration',
    'purchase_regularity', 'monetary_velocity',
    
    # ìµœê·¼ í™œë™
    'recent_activity_ratio_30d', 'recent_activity_ratio_60d', 'recent_activity_ratio_90d',
    'recent_monetary_ratio_30d', 'recent_monetary_ratio_60d', 'recent_monetary_ratio_90d',
    
    # êµ¬ë§¤ íŒ¨í„´
    'weekend_purchase_ratio', 'month_diversity',
    'unique_products', 'avg_products_per_order',
    
    # ìœ„í—˜ ì§€í‘œ
    'interval_risk', 'activity_risk', 'monetary_risk', 'risk_score',
    
    # ì„¸ê·¸ë¨¼íŠ¸
    'Segment_Encoded',
    
    # ê³ ê° ê°€ì¹˜
    'is_high_value', 'lifetime_value_per_day', 'purchase_efficiency'
]

# ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
available_features = [col for col in feature_cols if col in data.columns]
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ ìˆ˜: {len(available_features)}")

# ê²°ì¸¡ê°’ ì²˜ë¦¬
data[available_features] = data[available_features].fillna(0)

# ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
data[available_features] = data[available_features].replace([np.inf, -np.inf], 0)

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
X = data[available_features]
y = data['is_churned']

print(f"í”¼ì²˜ í–‰ë ¬ í¬ê¸°: {X.shape}")
print(f"íƒ€ê²Ÿ ë¶„í¬: {y.value_counts().to_dict()}")

# ========================================================================
# 6. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (ìˆ˜ì •)
# ========================================================================
print("\nğŸ“Š 6ë‹¨ê³„: ë°ì´í„° ë¶„í• ")
print("-" * 50)

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
X = data[available_features]
y = data['is_churned']

# ì „ì²´ ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ì„ì‹œ ì„¸íŠ¸(ê²€ì¦ + í…ŒìŠ¤íŠ¸)ë¡œ ë¶„í• 
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ì„ì‹œ ì„¸íŠ¸ë¥¼ ê²€ì¦ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"í•™ìŠµ ì„¸íŠ¸: {X_train.shape[0]}ê°œ (ì´íƒˆë¥ : {y_train.mean():.1%})")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]}ê°œ (ì´íƒˆë¥ : {y_val.mean():.1%})")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]}ê°œ (ì´íƒˆë¥ : {y_test.mean():.1%})")

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
# ========================================================================
# 7. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# ========================================================================
print("\nğŸ¤– 7ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
print("-" * 50)

models = {
    'Logistic Regression': LogisticRegression(
        class_weight='balanced',
        max_iter=1000,
        random_state=42
    ),
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        min_samples_split=20,
        random_state=42
    )
}

results = []
best_model = None
best_auc = 0

for model_name, model in models.items():
    print(f"\ní•™ìŠµ ì¤‘: {model_name}")
    
    # ëª¨ë¸ í•™ìŠµ
    if model_name == 'Logistic Regression':
        model.fit(X_train_scaled, y_train)
        y_pred_val = model.predict(X_val_scaled)
        y_proba_val = model.predict_proba(X_val_scaled)[:, 1]
        y_pred_test = model.predict(X_test_scaled)
        y_proba_test = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred_val = model.predict(X_val)
        y_proba_val = model.predict_proba(X_val)[:, 1]
        y_pred_test = model.predict(X_test)
        y_proba_test = model.predict_proba(X_test)[:, 1]
    
    # ì„±ëŠ¥ í‰ê°€
    val_auc = roc_auc_score(y_val, y_proba_val)
    test_auc = roc_auc_score(y_test, y_proba_test)
    
    # ì •ë°€ë„, ì¬í˜„ìœ¨ ê³„ì‚°
    from sklearn.metrics import precision_score, recall_score, f1_score
    precision = precision_score(y_test, y_pred_test)
    recall = recall_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)
    
    results.append({
        'Model': model_name,
        'Val_AUC': val_auc,
        'Test_AUC': test_auc,
        'Precision': precision,
        'Recall': recall,
        'F1': f1
    })
    
    print(f"  ê²€ì¦ AUC: {val_auc:.3f}")
    print(f"  í…ŒìŠ¤íŠ¸ AUC: {test_auc:.3f}")
    print(f"  ì •ë°€ë„: {precision:.3f}")
    print(f"  ì¬í˜„ìœ¨: {recall:.3f}")
    print(f"  F1 ì ìˆ˜: {f1:.3f}")
    
    if test_auc > best_auc:
        best_auc = test_auc
        best_model = model
        best_model_name = model_name
        best_y_proba = y_proba_test
        best_y_pred = y_pred_test

# ê²°ê³¼ ìš”ì•½
results_df = pd.DataFrame(results)
print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(results_df.round(3))

# ========================================================================
# 8. ìµœì  ëª¨ë¸ ìƒì„¸ ë¶„ì„
# ========================================================================
print(f"\nğŸ† 8ë‹¨ê³„: ìµœì  ëª¨ë¸ ìƒì„¸ ë¶„ì„ - {best_model_name}")
print("-" * 50)

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, best_y_pred)
print("\ní˜¼ë™ í–‰ë ¬:")
print(f"{'':10s} {'ì˜ˆì¸¡: í™œì„±':>12s} {'ì˜ˆì¸¡: ì´íƒˆ':>12s}")
print(f"{'ì‹¤ì œ: í™œì„±':10s} {cm[0,0]:12d} {cm[0,1]:12d}")
print(f"{'ì‹¤ì œ: ì´íƒˆ':10s} {cm[1,0]:12d} {cm[1,1]:12d}")

# í”¼ì²˜ ì¤‘ìš”ë„ (Tree ê¸°ë°˜ ëª¨ë¸ì˜ ê²½ìš°)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'Feature': available_features,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nìƒìœ„ 15ê°œ ì¤‘ìš” í”¼ì²˜:")
    print(feature_importance.head(15).to_string(index=False))

# ì„ê³„ê°’ ìµœì í™”
precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, best_y_proba)
f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)
optimal_idx = np.argmax(f1_scores[:-1])
optimal_threshold = thresholds[optimal_idx]

print(f"\nìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}")
print(f"ìµœì  F1 ì ìˆ˜: {f1_scores[optimal_idx]:.3f}")

# ========================================================================
# 9. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
# ========================================================================
print("\nğŸ’¼ 9ë‹¨ê³„: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸")
print("-" * 50)

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
if best_model_name == 'Logistic Regression':
    full_proba = best_model.predict_proba(scaler.transform(X))[:, 1]
else:
    full_proba = best_model.predict_proba(X)[:, 1]

data['churn_probability'] = full_proba

# ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜
data['risk_level'] = pd.cut(data['churn_probability'], 
                            bins=[0, 0.3, 0.5, 0.7, 1.0],
                            labels=['Low', 'Medium', 'High', 'Very High'])

risk_summary = data.groupby('risk_level').agg({
    'CustomerID': 'count',
    'Monetary': ['mean', 'sum'],
    'Frequency': 'mean',
    'is_churned': 'mean'
}).round(2)

print("\nìœ„í—˜ ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬:")
print(risk_summary)

# ê³ ìœ„í—˜ ê³ ê° ì‹ë³„
high_risk_customers = data[data['churn_probability'] >= 0.7]
print(f"\nğŸ”´ ê³ ìœ„í—˜ ê³ ê° (ì´íƒˆ í™•ë¥  â‰¥ 70%):")
print(f"- ê³ ê° ìˆ˜: {len(high_risk_customers):,}ëª…")
print(f"- í‰ê·  Frequency: {high_risk_customers['Frequency'].mean():.1f}")
print(f"- í‰ê·  Monetary: Â£{high_risk_customers['Monetary'].mean():.2f}")
print(f"- ì ì¬ ì†ì‹¤ ë§¤ì¶œ: Â£{high_risk_customers['Monetary'].sum():,.2f}")

# ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  ì´íƒˆ í™•ë¥ 
segment_risk = data.groupby('Segment').agg({
    'churn_probability': 'mean',
    'CustomerID': 'count'
}).sort_values('churn_probability', ascending=False)

print("\nì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  ì´íƒˆ í™•ë¥ :")
print(segment_risk.head(10).round(3))

# ========================================================================
# 10. ì‹œê°í™”
# ========================================================================
print("\nğŸ“Š 10ë‹¨ê³„: ì‹œê°í™” ìƒì„±")
print("-" * 50)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. ROC ê³¡ì„ 
from sklearn.metrics import roc_curve
ax = axes[0, 0]
fpr, tpr, _ = roc_curve(y_test, best_y_proba)
ax.plot(fpr, tpr, label=f'{best_model_name} (AUC={best_auc:.3f})', linewidth=2)
ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve', fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# 2. Precision-Recall ê³¡ì„ 
ax = axes[0, 1]
ax.plot(recall_vals[:-1], precision_vals[:-1], linewidth=2)
ax.scatter(recall_vals[optimal_idx], precision_vals[optimal_idx], 
          color='red', s=100, zorder=5, label=f'Optimal (F1={f1_scores[optimal_idx]:.3f})')
ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.set_title('Precision-Recall Curve', fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)
ax = axes[0, 2]
if hasattr(best_model, 'feature_importances_'):
    top_features = feature_importance.head(10)
    ax.barh(range(len(top_features)), top_features['Importance'].values, color='skyblue')
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features['Feature'].values)
    ax.set_xlabel('Importance')
    ax.set_title('Top 10 Feature Importance', fontweight='bold')
    ax.invert_yaxis()
else:
    ax.text(0.5, 0.5, 'Feature importance\nnot available\nfor this model', 
           ha='center', va='center', transform=ax.transAxes)
    ax.set_title('Feature Importance', fontweight='bold')

# 4. ì´íƒˆ í™•ë¥  ë¶„í¬
ax = axes[1, 0]
ax.hist(data[data['is_churned'] == 0]['churn_probability'], bins=30, 
        alpha=0.6, label='Active', color='green', edgecolor='black')
ax.hist(data[data['is_churned'] == 1]['churn_probability'], bins=30, 
        alpha=0.6, label='Churned', color='red', edgecolor='black')
ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Default Threshold')
ax.axvline(x=optimal_threshold, color='blue', linestyle='--', alpha=0.5, 
          label=f'Optimal Threshold ({optimal_threshold:.2f})')
ax.set_xlabel('Churn Probability')
ax.set_ylabel('Number of Customers')
ax.set_title('Churn Probability Distribution', fontweight='bold')
ax.legend()

# 5. ìœ„í—˜ ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬
ax = axes[1, 1]
risk_counts = data['risk_level'].value_counts()
colors = ['green', 'yellow', 'orange', 'red']
bars = ax.bar(range(len(risk_counts)), risk_counts.values, 
              color=[colors[i] for i in range(len(risk_counts))])
ax.set_xticks(range(len(risk_counts)))
ax.set_xticklabels(risk_counts.index)
ax.set_ylabel('Number of Customers')
ax.set_title('Customer Distribution by Risk Level', fontweight='bold')
for i, bar in enumerate(bars):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{int(height)}\n({height/len(data)*100:.1f}%)',
           ha='center', va='bottom')

# 6. ì‹œê³„ì—´ í”¼ì²˜ ìƒê´€ê´€ê³„
ax = axes[1, 2]
time_series_features = ['purchase_interval_trend', 'frequency_trend', 
                        'amount_trend_ratio', 'recent_activity_ratio_30d']
time_series_features = [f for f in time_series_features if f in data.columns]
if len(time_series_features) > 0:
    corr_with_churn = data[time_series_features + ['churn_probability']].corr()['churn_probability'][:-1]
    colors = ['red' if x > 0 else 'blue' for x in corr_with_churn.values]
    bars = ax.bar(range(len(corr_with_churn)), corr_with_churn.values, color=colors)
    ax.set_xticks(range(len(corr_with_churn)))
    ax.set_xticklabels(corr_with_churn.index, rotation=45, ha='right')
    ax.set_ylabel('Correlation with Churn Probability')
    ax.set_title('Time Series Features Correlation', fontweight='bold')
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    ax.grid(True, alpha=0.3, axis='y')

plt.suptitle(f'Churn Prediction Analysis - {best_model_name}', 
            fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('churn_prediction_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# ========================================================================
# 11. ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
# ========================================================================
print("\nğŸ¯ 11ë‹¨ê³„: ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸")
print("-" * 50)

# 1. ì¡°ê¸° ê²½ê³  ì‹ í˜¸
print("\nâš ï¸ ì¡°ê¸° ê²½ê³  ì‹ í˜¸:")
early_warning = data[
    (data['purchase_interval_trend'] > 20) & 
    (data['recent_activity_ratio_30d'] < 0.2) &
    (data['is_churned'] == 0)
]
print(f"- êµ¬ë§¤ ê°„ê²© ì¦ê°€ + ìµœê·¼ í™œë™ ê°ì†Œ: {len(early_warning):,}ëª…")
print(f"  í‰ê·  ì´íƒˆ í™•ë¥ : {early_warning['churn_probability'].mean():.1%}")

# 2. ì„¸ê·¸ë¨¼íŠ¸ë³„ ì•¡ì…˜ í”Œëœ
print("\nğŸ“‹ ì„¸ê·¸ë¨¼íŠ¸ë³„ ì•¡ì…˜ í”Œëœ:")

action_plans = {
    'At Risk': {
        'action': 'ê¸´ê¸‰ ì¬ì°¸ì—¬ ìº í˜ì¸',
        'tactics': ['ê°œì¸í™”ëœ í• ì¸ ì¿ í°', 'ë…ì  ì œí’ˆ ë¯¸ë¦¬ë³´ê¸°', 'ë¬´ë£Œ ë°°ì†¡']
    },
    'Cannot Lose Them': {
        'action': '1:1 ê³ ê° ê´€ë¦¬',
        'tactics': ['ì „ë‹´ ë§¤ë‹ˆì € ë°°ì •', 'VIP í˜œíƒ', 'ë§ì¶¤í˜• ìƒí’ˆ ì¶”ì²œ']
    },
    'Hibernating': {
        'action': 'Win-back ìº í˜ì¸',
        'tactics': ['ë³µê·€ í• ì¸', 'ì‹ ì œí’ˆ ì•Œë¦¼', 'ë¸Œëœë“œ ìŠ¤í† ë¦¬ ê³µìœ ']
    },
    'About to Sleep': {
        'action': 'ê´€ì‹¬ ìœ ë„',
        'tactics': ['ì´ë©”ì¼ ë¹ˆë„ ì¡°ì •', 'ê´€ì‹¬ì‚¬ ê¸°ë°˜ ì½˜í…ì¸ ', 'ì†Œì…œ ë¯¸ë””ì–´ íƒ€ê²ŸíŒ…']
    }
}

for segment, plan in action_plans.items():
    segment_data = data[data['Segment'] == segment]
    if len(segment_data) > 0:
        print(f"\n{segment} ({len(segment_data):,}ëª…, í‰ê·  ì´íƒˆ í™•ë¥ : {segment_data['churn_probability'].mean():.1%}):")
        print(f"  â†’ {plan['action']}")
        for tactic in plan['tactics']:
            print(f"    â€¢ {tactic}")

# 3. ë¹„ìš©-íš¨ê³¼ ë¶„ì„
print("\nğŸ’° ë¹„ìš©-íš¨ê³¼ ë¶„ì„:")

# ê°€ì •: ì¬ì°¸ì—¬ ìº í˜ì¸ ë¹„ìš© Â£5/ê³ ê°, ì„±ê³µë¥  25%, í‰ê·  ì¬í™œì„±í™” ê°€ì¹˜ = í‰ê·  AOV
campaign_cost_per_customer = 5
reactivation_success_rate = 0.25
avg_reactivation_value = data['avg_order_value'].mean()

for threshold in [0.5, 0.6, 0.7]:
    target_customers = data[
        (data['churn_probability'] >= threshold) & 
        (data['is_churned'] == 0)
    ]
    
    expected_cost = len(target_customers) * campaign_cost_per_customer
    expected_reactivations = len(target_customers) * reactivation_success_rate
    expected_revenue = expected_reactivations * avg_reactivation_value
    expected_roi = ((expected_revenue - expected_cost) / expected_cost * 100) if expected_cost > 0 else 0
    
    print(f"\nì„ê³„ê°’ {threshold:.0%}:")
    print(f"  - ëŒ€ìƒ ê³ ê°: {len(target_customers):,}ëª…")
    print(f"  - ì˜ˆìƒ ë¹„ìš©: Â£{expected_cost:,.0f}")
    print(f"  - ì˜ˆìƒ ì¬í™œì„±í™”: {expected_reactivations:.0f}ëª…")
    print(f"  - ì˜ˆìƒ ìˆ˜ìµ: Â£{expected_revenue:,.0f}")
    print(f"  - ì˜ˆìƒ ROI: {expected_roi:.1f}%")

# ========================================================================
# 12. ê²°ê³¼ ì €ì¥
# ========================================================================
print("\nğŸ’¾ 12ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
print("-" * 50)

# 1. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
prediction_results = data[['CustomerID', 'Segment', 'Recency', 'Frequency', 'Monetary',
                          'is_churned', 'churn_probability', 'risk_level']].copy()
prediction_results = prediction_results.sort_values('churn_probability', ascending=False)
prediction_results.to_csv('churn_predictions.csv', index=False)
print("âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: churn_predictions.csv")

# 2. ê³ ìœ„í—˜ ê³ ê° ë¦¬ìŠ¤íŠ¸
high_risk_export = high_risk_customers[['CustomerID', 'Segment', 'churn_probability',
                                        'Recency', 'Frequency', 'Monetary',
                                        'recent_activity_ratio_30d',
                                        'purchase_interval_trend']].copy()
high_risk_export.to_csv('high_risk_customers_prediction.csv', index=False)
print(f"âœ… ê³ ìœ„í—˜ ê³ ê° ë¦¬ìŠ¤íŠ¸ ì €ì¥: high_risk_customers_prediction.csv ({len(high_risk_export)}ëª…)")

# 3. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
model_performance = pd.DataFrame(results)
model_performance.to_csv('model_performance.csv', index=False)
print("âœ… ëª¨ë¸ ì„±ëŠ¥ ì €ì¥: model_performance.csv")

# 4. í”¼ì²˜ ì¤‘ìš”ë„ (Tree ê¸°ë°˜ ëª¨ë¸ì˜ ê²½ìš°)
if hasattr(best_model, 'feature_importances_'):
    feature_importance.to_csv('feature_importance.csv', index=False)
    print("âœ… í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥: feature_importance.csv")

# 5. ì„¸ê·¸ë¨¼íŠ¸ë³„ ìœ„í—˜ë„ ë¶„ì„
segment_risk_analysis = data.groupby('Segment').agg({
    'CustomerID': 'count',
    'churn_probability': ['mean', 'std'],
    'is_churned': 'mean',
    'Monetary': ['mean', 'sum'],
    'recent_activity_ratio_30d': 'mean',
    'purchase_interval_trend': 'mean'
}).round(3)
segment_risk_analysis.columns = ['_'.join(col).strip() for col in segment_risk_analysis.columns]
segment_risk_analysis.to_csv('segment_risk_analysis.csv')
print("âœ… ì„¸ê·¸ë¨¼íŠ¸ ìœ„í—˜ë„ ë¶„ì„ ì €ì¥: segment_risk_analysis.csv")

# ========================================================================
# 13. ìµœì¢… ìš”ì•½
# ========================================================================
print("\n" + "=" * 80)
print("ğŸ¯ ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ - ì—…ê³„ í‘œì¤€ ì ìš©")
print("=" * 80)

print("\nğŸ“Š ë°ì´í„° ìš”ì•½:")
print(f"- ì´ ê³ ê° ìˆ˜: {len(data):,}ëª…")
print(f"- ì´íƒˆë¥ : {data['is_churned'].mean():.1%}")
print(f"- ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(available_features)}")

print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥:")
print(f"- ìµœì  ëª¨ë¸: {best_model_name}")
print(f"- í…ŒìŠ¤íŠ¸ AUC: {best_auc:.3f}")
print(f"- ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f}")

print("\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
if hasattr(best_model, 'feature_importances_'):
    top3_features = feature_importance.head(3)
    print("ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ë³€ìˆ˜:")
    for idx, row in top3_features.iterrows():
        print(f"  {idx+1}. {row['Feature']}: {row['Importance']:.3f}")

print("\nğŸš€ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:")
print(f"- ê³ ìœ„í—˜ ê³ ê°: {len(high_risk_customers):,}ëª…")
print(f"- ì ì¬ ë°©ì–´ ê°€ëŠ¥ ë§¤ì¶œ: Â£{high_risk_customers['Monetary'].sum():,.0f}")
print(f"- ì¡°ê¸° ê²½ê³  ëŒ€ìƒ: {len(early_warning):,}ëª…")

print("\nğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„:")
print("1. ê³ ìœ„í—˜ ê³ ê° ëŒ€ìƒ ì¬ì°¸ì—¬ ìº í˜ì¸ ì‹¤í–‰")
print("2. A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ê°œì… ì „ëµ ê²€ì¦")
print("3. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì •ê¸°ì  ì¬í•™ìŠµ")
print("4. ì‹¤ì‹œê°„ ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ êµ¬ì¶•")

print("\nâœ¨ ì‹œê³„ì—´ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ì˜ˆì¸¡ ëª¨ë¸ ì™„ì„±!")
print("=" * 80)