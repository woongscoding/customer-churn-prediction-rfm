import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ëª¨ë¸ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (roc_auc_score, roc_curve, classification_report, 
                           confusion_matrix, precision_recall_curve, f1_score,
                           precision_score, recall_score, accuracy_score)
from imblearn.over_sampling import SMOTE
import pickle

# XGBoostì™€ LightGBM ê°€ìš©ì„± í™•ì¸
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
    print("âš ï¸ XGBoost not installed. Install with: pip install xgboost")

try:
    import lightgbm as lgb
    LGB_AVAILABLE = True
except ImportError:
    LGB_AVAILABLE = False
    print("âš ï¸ LightGBM not installed. Install with: pip install lightgbm")

print("=" * 80)
print("   í†µí•© ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸")
print("   RFM + ì‹œê³„ì—´ í”¼ì²˜ + ëª¨ë¸ ì•™ìƒë¸”")
print("=" * 80)

# ========================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° í†µí•©
# ========================================================================
print("\nğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° í†µí•©")
print("-" * 50)

# RFM ë¶„ì„ ê²°ê³¼ ë¡œë“œ
try:
    rfm = pd.read_csv('results/rfm_result/rfm_analysis_results.csv')
    print(f"âœ… RFM ë°ì´í„° ë¡œë“œ: {rfm.shape}")
except FileNotFoundError:
    print("âŒ rfm_analysis_results.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ë¨¼ì € rfm_analysis.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    exit()

# ì›ë³¸ ê±°ë˜ ë°ì´í„° ë¡œë“œ (ì‹œê³„ì—´ í”¼ì²˜ ìƒì„±ìš©)
try:
    df = pd.read_csv('data/processed/cleaned_retail_data.csv', encoding='ISO-8859-1')
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    df['InvoiceNo'] = df['InvoiceNo'].astype(str)
    print(f"âœ… ê±°ë˜ ë°ì´í„° ë¡œë“œ: {df.shape}")
except FileNotFoundError:
    # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
    try:
        df = pd.read_csv('data/processed/cleaned_retail_data.csv', encoding='ISO-8859-1')
        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
        df['InvoiceNo'] = df['InvoiceNo'].astype(str)
        print(f"âœ… ê±°ë˜ ë°ì´í„° ë¡œë“œ (ëŒ€ì²´ ê²½ë¡œ): {df.shape}")
    except:
        print("âŒ ê±°ë˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

# ì •ìƒ ê±°ë˜ë§Œ í•„í„°ë§
df_clean = df[
    (df['Quantity'] > 0) & 
    (df['UnitPrice'] > 0) & 
    (~df['InvoiceNo'].str.contains('C', na=False)) &
    (df['CustomerID'].notna())
].copy()
df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['UnitPrice']

# ë¶„ì„ ê¸°ì¤€ì¼
analysis_date = df_clean['InvoiceDate'].max() + timedelta(days=1)
print(f"ë¶„ì„ ê¸°ì¤€ì¼: {analysis_date}")

# ========================================================================
# 2. ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ
# ========================================================================
print("\nâ° 2ë‹¨ê³„: ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ")
print("-" * 50)

def extract_time_features(customer_id, transactions_df, analysis_date):
    """
    ë„ë©”ì¸ ì§€ì‹: êµ¬ë§¤ íŒ¨í„´ì˜ ì‹œê°„ì  ë³€í™”ë¥¼ í¬ì°©í•˜ëŠ” í”¼ì²˜
    """
    customer_trans = transactions_df[transactions_df['CustomerID'] == customer_id].sort_values('InvoiceDate')
    
    features = {}
    
    # ê¸°ë³¸ í†µê³„
    features['total_transactions'] = len(customer_trans)
    features['unique_purchase_days'] = customer_trans['InvoiceDate'].dt.date.nunique()
    features['unique_products'] = customer_trans['StockCode'].nunique()
    
    if len(customer_trans) < 2:
        # ë‹¨ì¼ êµ¬ë§¤ ê³ ê°
        features['avg_days_between_purchases'] = 999  # í° ê°’ìœ¼ë¡œ ì„¤ì •
        features['std_days_between_purchases'] = 0
        features['purchase_interval_trend'] = 0
        features['monetary_trend'] = 0
        features['frequency_trend'] = 0
        features['purchase_regularity'] = 0
        for days in [30, 60, 90]:
            features[f'recent_activity_ratio_{days}d'] = 0
            features[f'recent_monetary_ratio_{days}d'] = 0
        return features
    
    # êµ¬ë§¤ ê°„ê²© ë¶„ì„
    purchase_dates = pd.to_datetime(customer_trans.groupby(
        customer_trans['InvoiceDate'].dt.date)['InvoiceDate'].first())
    
    if len(purchase_dates) > 1:
        intervals = np.diff(purchase_dates.values).astype('timedelta64[D]').astype(float)
        features['avg_days_between_purchases'] = np.mean(intervals)
        features['std_days_between_purchases'] = np.std(intervals)
        
        # êµ¬ë§¤ ê°„ê²© íŠ¸ë Œë“œ (ì¦ê°€í•˜ë©´ ì–‘ìˆ˜)
        if len(intervals) > 1:
            features['purchase_interval_trend'] = np.polyfit(range(len(intervals)), intervals, 1)[0]
        else:
            features['purchase_interval_trend'] = 0
            
        # êµ¬ë§¤ ê·œì¹™ì„± (ë³€ë™ê³„ìˆ˜ì˜ ì—­ìˆ˜)
        if features['avg_days_between_purchases'] > 0:
            cv = features['std_days_between_purchases'] / features['avg_days_between_purchases']
            features['purchase_regularity'] = 1 / (1 + cv)
        else:
            features['purchase_regularity'] = 0
    else:
        features['avg_days_between_purchases'] = 999
        features['std_days_between_purchases'] = 0
        features['purchase_interval_trend'] = 0
        features['purchase_regularity'] = 0
    
    # ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì „ë°˜ê¸° vs í›„ë°˜ê¸°)
    time_span = (customer_trans['InvoiceDate'].max() - customer_trans['InvoiceDate'].min()).days
    if time_span > 0:
        mid_date = customer_trans['InvoiceDate'].min() + timedelta(days=time_span/2)
        
        first_half = customer_trans[customer_trans['InvoiceDate'] <= mid_date]
        second_half = customer_trans[customer_trans['InvoiceDate'] > mid_date]
        
        # ê¸ˆì•¡ íŠ¸ë Œë“œ
        if first_half['TotalAmount'].sum() > 0:
            features['monetary_trend'] = (second_half['TotalAmount'].sum() / 
                                         first_half['TotalAmount'].sum()) - 1
        else:
            features['monetary_trend'] = 0
            
        # ë¹ˆë„ íŠ¸ë Œë“œ
        if len(first_half) > 0:
            features['frequency_trend'] = (len(second_half) / len(first_half)) - 1
        else:
            features['frequency_trend'] = 0
    else:
        features['monetary_trend'] = 0
        features['frequency_trend'] = 0
    
    # ìµœê·¼ í™œë™ ë¹„ìœ¨
    for days in [30, 60, 90]:
        recent_date = analysis_date - timedelta(days=days)
        recent_trans = customer_trans[customer_trans['InvoiceDate'] >= recent_date]
        
        features[f'recent_activity_ratio_{days}d'] = len(recent_trans) / len(customer_trans)
        
        if customer_trans['TotalAmount'].sum() > 0:
            features[f'recent_monetary_ratio_{days}d'] = (recent_trans['TotalAmount'].sum() / 
                                                          customer_trans['TotalAmount'].sum())
        else:
            features[f'recent_monetary_ratio_{days}d'] = 0
    
    return features

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ
print("ì‹œê³„ì—´ í”¼ì²˜ ì¶”ì¶œ ì¤‘...")
time_features_list = []
batch_size = 500
customer_ids = rfm['CustomerID'].values

for i in range(0, len(customer_ids), batch_size):
    batch_ids = customer_ids[i:i+batch_size]
    
    for customer_id in batch_ids:
        features = extract_time_features(customer_id, df_clean, analysis_date)
        features['CustomerID'] = customer_id
        time_features_list.append(features)
    
    if (i + batch_size) % 2000 == 0:
        print(f"  ì²˜ë¦¬ ì™„ë£Œ: {min(i + batch_size, len(customer_ids))}/{len(customer_ids)}")

time_features_df = pd.DataFrame(time_features_list)
print(f"âœ… ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {time_features_df.shape}")

# RFMê³¼ ì‹œê³„ì—´ í”¼ì²˜ í†µí•©
data = rfm.merge(time_features_df, on='CustomerID', how='left')
print(f"âœ… í†µí•© ë°ì´í„°: {data.shape}")

# ========================================================================
# 3. ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ========================================================================
print("\nğŸ”§ 3ë‹¨ê³„: ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: RFM ìƒí˜¸ì‘ìš© í”¼ì²˜ (Recency ì œì™¸ - ë¦¬í‚¤ì§€ ë°©ì§€)
data['FM_Score'] = data['F_Score'] * data['M_Score']
data['F_Score_squared'] = data['F_Score'] ** 2
data['M_Score_squared'] = data['M_Score'] ** 2

# ì´íƒˆ ìœ„í—˜ ì§€í‘œ ìƒì„±
# ë„ë©”ì¸ ì§€ì‹: êµ¬ë§¤ íŒ¨í„´ ì•…í™” ì‹ í˜¸
data['interval_risk'] = (data['purchase_interval_trend'] > 10).astype(int)  # ê°„ê²© 10ì¼ ì´ìƒ ì¦ê°€
data['activity_risk'] = (data['recent_activity_ratio_30d'] < 0.1).astype(int)  # ìµœê·¼ 30ì¼ í™œë™ 10% ë¯¸ë§Œ
data['monetary_risk'] = (data['monetary_trend'] < -0.3).astype(int)  # ê¸ˆì•¡ 30% ì´ìƒ ê°ì†Œ
data['frequency_risk'] = (data['frequency_trend'] < -0.3).astype(int)  # ë¹ˆë„ 30% ì´ìƒ ê°ì†Œ

# ì¢…í•© ìœ„í—˜ ì ìˆ˜
data['risk_score'] = (data['interval_risk'] + data['activity_risk'] + 
                      data['monetary_risk'] + data['frequency_risk'])

# ì„¸ê·¸ë¨¼íŠ¸ ìœ„í—˜ë„ ì¸ì½”ë”©
# ë„ë©”ì¸ ì§€ì‹: ì„¸ê·¸ë¨¼íŠ¸ë³„ ì´íƒˆ ìœ„í—˜ë„ ë°˜ì˜
segment_risk_mapping = {
    'Champions': 1,
    'Loyal Customers': 2,
    'Potential Loyalists': 3,
    'Promising': 3,
    'New Customers': 4,
    'Need Attention': 5,
    'About to Sleep': 6,
    'At Risk': 7,
    'Cannot Lose Them': 8,
    'Hibernating': 8,
    'Lost': 9,
    'Others': 5
}
data['segment_risk_level'] = data['Segment'].map(segment_risk_mapping).fillna(5)

# ê³ ê° ê°€ì¹˜ ì§€í‘œ
data['customer_value_score'] = data['F_Score'] * 0.3 + data['M_Score'] * 0.7  # ê¸ˆì•¡ì— ë” ê°€ì¤‘ì¹˜
data['avg_order_value'] = data['Monetary'] / (data['Frequency'] + 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

# ìƒì•  ê°€ì¹˜ ê´€ë ¨
data['lifetime_days'] = data.apply(
    lambda x: (analysis_date - df_clean[df_clean['CustomerID'] == x['CustomerID']]['InvoiceDate'].min()).days 
    if pd.notna(x['CustomerID']) else 0, axis=1
)
data['lifetime_value_per_day'] = data['Monetary'] / (data['lifetime_days'] + 1)

print(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: ì´ {len(data.columns)}ê°œ í”¼ì²˜")

# ========================================================================
# 4. ì´íƒˆ ë¼ë²¨ í™•ì¸ (120ì¼ ê¸°ì¤€)
# ========================================================================
print("\nğŸ¯ 4ë‹¨ê³„: ì´íƒˆ ë¼ë²¨ í™•ì¸")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: 120ì¼(4ê°œì›”) ì´íƒˆ ê¸°ì¤€
CHURN_THRESHOLD = 120

# RFM ë¶„ì„ì—ì„œ ì´ë¯¸ ì •ì˜ëœ is_churned í™•ì¸
if 'is_churned' not in data.columns:
    data['is_churned'] = (data['Recency'] > CHURN_THRESHOLD).astype(int)

print(f"ì´íƒˆ ê¸°ì¤€: {CHURN_THRESHOLD}ì¼")
print(f"ì „ì²´ ì´íƒˆë¥ : {data['is_churned'].mean():.1%}")
print(f"- í™œì„± ê³ ê°: {(data['is_churned'] == 0).sum():,}ëª…")
print(f"- ì´íƒˆ ê³ ê°: {(data['is_churned'] == 1).sum():,}ëª…")
print(f"í´ë˜ìŠ¤ ë¹„ìœ¨: 1:{(data['is_churned'] == 0).sum() / max((data['is_churned'] == 1).sum(), 1):.1f}")

# ========================================================================
# 5. í”¼ì²˜ ì„ íƒ (ë¦¬í‚¤ì§€ ë°©ì§€)
# ========================================================================
print("\nğŸ“ 5ë‹¨ê³„: í”¼ì²˜ ì„ íƒ (ë¦¬í‚¤ì§€ ë°©ì§€)")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: Recency ë° ê´€ë ¨ í”¼ì²˜ ì œì™¸ (íƒ€ê²Ÿ ë¦¬í‚¤ì§€ ë°©ì§€)
feature_cols = [
    # RFM ê¸°ë³¸ (Recency ì œì™¸)
    'Frequency', 'Monetary',
    'F_Score', 'M_Score',
    
    # RFM íŒŒìƒ
    'FM_Score', 'F_Score_squared', 'M_Score_squared',
    'customer_value_score', 'avg_order_value',
    
    # ì‹œê³„ì—´ í”¼ì²˜
    'total_transactions', 'unique_purchase_days', 'unique_products',
    'avg_days_between_purchases', 'std_days_between_purchases',
    'purchase_interval_trend', 'monetary_trend', 'frequency_trend',
    'purchase_regularity',
    
    # ìµœê·¼ í™œë™
    'recent_activity_ratio_30d', 'recent_activity_ratio_60d', 'recent_activity_ratio_90d',
    'recent_monetary_ratio_30d', 'recent_monetary_ratio_60d', 'recent_monetary_ratio_90d',
    
    # ìœ„í—˜ ì§€í‘œ
    'interval_risk', 'activity_risk', 'monetary_risk', 'frequency_risk', 'risk_score',
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë° ê°€ì¹˜
    'segment_risk_level', 'is_high_value',
    'lifetime_value_per_day'
]

# ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
available_features = [col for col in feature_cols if col in data.columns]
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ ìˆ˜: {len(available_features)}")

# ê²°ì¸¡ê°’ ë° ë¬´í•œëŒ€ ì²˜ë¦¬
data[available_features] = data[available_features].fillna(0)
data[available_features] = data[available_features].replace([np.inf, -np.inf], 0)
# ========================================================================
# 6. ë°ì´í„° ë¶„í• 
# ========================================================================
print("\nğŸ“Š 6ë‹¨ê³„: ë°ì´í„° ë¶„í• ")
print("-" * 50)

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
X = data[available_features]
y = data['is_churned']

# ì „ì²´ ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ì„ì‹œ ì„¸íŠ¸(ê²€ì¦ + í…ŒìŠ¤íŠ¸)ë¡œ ë¶„í• 
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ì„ì‹œ ì„¸íŠ¸ë¥¼ ê²€ì¦ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"í•™ìŠµ ì„¸íŠ¸: {X_train.shape[0]}ê°œ (ì´íƒˆë¥ : {y_train.mean():.1%})")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]}ê°œ (ì´íƒˆë¥ : {y_val.mean():.1%})")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]}ê°œ (ì´íƒˆë¥ : {y_test.mean():.1%})")

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¶”ê°€
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)


# ========================================================================
# 7. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
# ========================================================================
print("\nâš–ï¸ 7ë‹¨ê³„: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬")
print("-" * 50)

# SMOTE ì ìš© (í›ˆë ¨ ë°ì´í„°ë§Œ)
if y_train.mean() < 0.4:  # ì´íƒˆë¥ ì´ 40% ë¯¸ë§Œì¼ ë•Œë§Œ
    print("SMOTE ì ìš© ì¤‘...")
    smote = SMOTE(random_state=42, sampling_strategy=0.5)  # 50%ê¹Œì§€ë§Œ ì¦ê°€
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
    print(f"ê· í˜•í™” í›„ í›ˆë ¨ ì„¸íŠ¸: {X_train_balanced.shape[0]}ê°œ (ì´íƒˆë¥ : {y_train_balanced.mean():.1%})")
else:
    X_train_balanced = X_train_scaled
    y_train_balanced = y_train

# ========================================================================
# 8. ê°œë³„ ëª¨ë¸ í•™ìŠµ
# ========================================================================
print("\nğŸ¤– 8ë‹¨ê³„: ê°œë³„ ëª¨ë¸ í•™ìŠµ")
print("-" * 50)

models = {}
model_scores = {}

# 1. Logistic Regression
print("\n[1/5] Logistic Regression í•™ìŠµ ì¤‘...")
lr_model = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42,
    solver='liblinear'
)
lr_model.fit(X_train_balanced, y_train_balanced)
lr_val_pred = lr_model.predict_proba(X_val_scaled)[:, 1]
lr_val_score = roc_auc_score(y_val, lr_val_pred)
models['LogisticRegression'] = lr_model
model_scores['LogisticRegression'] = lr_val_score
print(f"  ê²€ì¦ AUC: {lr_val_score:.4f}")

# 2. Random Forest
print("\n[2/5] Random Forest í•™ìŠµ ì¤‘...")
rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)  # ì›ë³¸ ë°ì´í„° ì‚¬ìš©
rf_val_pred = rf_model.predict_proba(X_val)[:, 1]
rf_val_score = roc_auc_score(y_val, rf_val_pred)
models['RandomForest'] = rf_model
model_scores['RandomForest'] = rf_val_score
print(f"  ê²€ì¦ AUC: {rf_val_score:.4f}")

# 3. Gradient Boosting
print("\n[3/5] Gradient Boosting í•™ìŠµ ì¤‘...")
gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=20,
    random_state=42
)
gb_model.fit(X_train, y_train)
gb_val_pred = gb_model.predict_proba(X_val)[:, 1]
gb_val_score = roc_auc_score(y_val, gb_val_pred)
models['GradientBoosting'] = gb_model
model_scores['GradientBoosting'] = gb_val_score
print(f"  ê²€ì¦ AUC: {gb_val_score:.4f}")

# 4. XGBoost (ê°€ëŠ¥í•œ ê²½ìš°)
if XGB_AVAILABLE:
    print("\n[4/5] XGBoost í•™ìŠµ ì¤‘...")
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°€ì¤‘ì¹˜
    pos_weight = (y_train == 0).sum() / max((y_train == 1).sum(), 1)
    
    xgb_model = xgb.XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        scale_pos_weight=pos_weight,
        random_state=42,
        eval_metric='logloss',
        verbosity=0
    )
    xgb_model.fit(X_train, y_train)
    xgb_val_pred = xgb_model.predict_proba(X_val)[:, 1]
    xgb_val_score = roc_auc_score(y_val, xgb_val_pred)
    models['XGBoost'] = xgb_model
    model_scores['XGBoost'] = xgb_val_score
    print(f"  ê²€ì¦ AUC: {xgb_val_score:.4f}")

# 5. LightGBM (ê°€ëŠ¥í•œ ê²½ìš°)
if LGB_AVAILABLE:
    print("\n[5/5] LightGBM í•™ìŠµ ì¤‘...")
    lgb_model = lgb.LGBMClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        num_leaves=31,
        class_weight='balanced',
        random_state=42,
        verbosity=-1
    )
    lgb_model.fit(X_train, y_train)
    lgb_val_pred = lgb_model.predict_proba(X_val)[:, 1]
    lgb_val_score = roc_auc_score(y_val, lgb_val_pred)
    models['LightGBM'] = lgb_model
    model_scores['LightGBM'] = lgb_val_score
    print(f"  ê²€ì¦ AUC: {lgb_val_score:.4f}")

# ========================================================================
# 9. ëª¨ë¸ ì•™ìƒë¸”
# ========================================================================
print("\nğŸ­ 9ë‹¨ê³„: ëª¨ë¸ ì•™ìƒë¸”")
print("-" * 50)

# ìƒìœ„ 3ê°œ ëª¨ë¸ ì„ íƒ
top_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)[:3]
print("\nìƒìœ„ 3ê°œ ëª¨ë¸:")
for model_name, score in top_models:
    print(f"  - {model_name}: AUC {score:.4f}")

# Soft Voting ì•™ìƒë¸”
print("\nì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...")
ensemble_estimators = []
for model_name, _ in top_models:
    if model_name == 'LogisticRegression':
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ í•„ìš”
        from sklearn.pipeline import Pipeline
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', models[model_name])
        ])
        ensemble_estimators.append((model_name, pipeline))
    else:
        ensemble_estimators.append((model_name, models[model_name]))

ensemble_model = VotingClassifier(
    estimators=ensemble_estimators,
    voting='soft',
    n_jobs=-1
)

# ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
ensemble_model.fit(X_train, y_train)
ensemble_val_pred = ensemble_model.predict_proba(X_val)[:, 1]
ensemble_val_score = roc_auc_score(y_val, ensemble_val_pred)
print(f"\nì•™ìƒë¸” ëª¨ë¸ ê²€ì¦ AUC: {ensemble_val_score:.4f}")

# ========================================================================
# 10. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° í‰ê°€
# ========================================================================
print("\nğŸ† 10ë‹¨ê³„: ìµœì¢… ëª¨ë¸ ì„ íƒ ë° í‰ê°€")
print("-" * 50)

# ëª¨ë“  ëª¨ë¸ ë¹„êµ (ì•™ìƒë¸” í¬í•¨)
all_scores = model_scores.copy()
all_scores['Ensemble'] = ensemble_val_score

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_model_name = max(all_scores, key=all_scores.get)
best_score = all_scores[best_model_name]

print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (ê²€ì¦ AUC: {best_score:.4f})")

# ìµœì¢… ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
if best_model_name == 'Ensemble':
    final_model = ensemble_model
    test_pred = final_model.predict(X_test)
    test_proba = final_model.predict_proba(X_test)[:, 1]
elif best_model_name == 'LogisticRegression':
    final_model = models[best_model_name]
    test_pred = final_model.predict(X_test_scaled)
    test_proba = final_model.predict_proba(X_test_scaled)[:, 1]
else:
    final_model = models[best_model_name]
    test_pred = final_model.predict(X_test)
    test_proba = final_model.predict_proba(X_test)[:, 1]

# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥ í‰ê°€
test_auc = roc_auc_score(y_test, test_proba)
test_precision = precision_score(y_test, test_pred)
test_recall = recall_score(y_test, test_pred)
test_f1 = f1_score(y_test, test_pred)
test_accuracy = accuracy_score(y_test, test_pred)

print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥:")
print(f"  - AUC: {test_auc:.4f}")
print(f"  - Accuracy: {test_accuracy:.4f}")
print(f"  - Precision: {test_precision:.4f}")
print(f"  - Recall: {test_recall:.4f}")
print(f"  - F1-Score: {test_f1:.4f}")

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, test_pred)
print("\ní˜¼ë™ í–‰ë ¬:")
print(f"{'':10s} {'ì˜ˆì¸¡: í™œì„±':>12s} {'ì˜ˆì¸¡: ì´íƒˆ':>12s}")
print(f"{'ì‹¤ì œ: í™œì„±':10s} {cm[0,0]:12d} {cm[0,1]:12d}")
print(f"{'ì‹¤ì œ: ì´íƒˆ':10s} {cm[1,0]:12d} {cm[1,1]:12d}")

# ========================================================================
# 11. êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ì¸
# ========================================================================
print("\nğŸ” 11ë‹¨ê³„: êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì•ˆì •ì„± í™•ì¸")
print("-" * 50)

# ì „ì²´ ë°ì´í„°ë¡œ êµì°¨ ê²€ì¦
X_all = data[available_features]
y_all = data['is_churned']

cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

print("5-Fold êµì°¨ ê²€ì¦ ì¤‘...")
if best_model_name == 'LogisticRegression':
    cv_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', models[best_model_name])
    ])
    cv_scores = cross_val_score(cv_pipeline, X_all, y_all, cv=cv_folds, scoring='roc_auc', n_jobs=-1)
elif best_model_name == 'Ensemble':
    cv_scores = cross_val_score(ensemble_model, X_all, y_all, cv=cv_folds, scoring='roc_auc', n_jobs=-1)
else:
    cv_scores = cross_val_score(models[best_model_name], X_all, y_all, cv=cv_folds, scoring='roc_auc', n_jobs=-1)

print(f"\n{best_model_name} êµì°¨ ê²€ì¦ ê²°ê³¼:")
print(f"  - í‰ê·  AUC: {cv_scores.mean():.4f}")
print(f"  - í‘œì¤€í¸ì°¨: {cv_scores.std():.4f}")
print(f"  - ìµœì†Œ AUC: {cv_scores.min():.4f}")
print(f"  - ìµœëŒ€ AUC: {cv_scores.max():.4f}")

# ========================================================================
# 12. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
# ========================================================================
print("\nğŸ“ˆ 12ë‹¨ê³„: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„")
print("-" * 50)

# Tree ê¸°ë°˜ ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„
if best_model_name in ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM']:
    feature_importance = pd.DataFrame({
        'Feature': available_features,
        'Importance': models[best_model_name].feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nìƒìœ„ 15ê°œ ì¤‘ìš” í”¼ì²˜:")
    print(feature_importance.head(15).to_string(index=False))
    
elif best_model_name == 'LogisticRegression':
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ ê³„ìˆ˜
    feature_coef = pd.DataFrame({
        'Feature': available_features,
        'Coefficient': models[best_model_name].coef_[0],
        'Abs_Coefficient': np.abs(models[best_model_name].coef_[0])
    }).sort_values('Abs_Coefficient', ascending=False)
    
    print("\nìƒìœ„ 15ê°œ ì¤‘ìš” í”¼ì²˜ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
    print(feature_coef[['Feature', 'Coefficient']].head(15).to_string(index=False))
    
elif best_model_name == 'Ensemble':
    print("ì•™ìƒë¸” ëª¨ë¸ì€ ê°œë³„ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    # ëŒ€ì‹  ê°œë³„ ëª¨ë¸ë“¤ì˜ í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
    importance_sum = np.zeros(len(available_features))
    importance_count = 0
    
    for model_name, _ in top_models:
        if model_name in ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM']:
            importance_sum += models[model_name].feature_importances_
            importance_count += 1
    
    if importance_count > 0:
        avg_importance = importance_sum / importance_count
        feature_importance = pd.DataFrame({
            'Feature': available_features,
            'Avg_Importance': avg_importance
        }).sort_values('Avg_Importance', ascending=False)
        
        print("\nì•™ìƒë¸” êµ¬ì„± ëª¨ë¸ë“¤ì˜ í‰ê·  í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 15ê°œ):")
        print(feature_importance.head(15).to_string(index=False))

# ========================================================================
# 13. ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ë° ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜
# ========================================================================
print("\nğŸ¯ 13ë‹¨ê³„: ì „ì²´ ê³ ê° ì´íƒˆ í™•ë¥  ì˜ˆì¸¡")
print("-" * 50)

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
if best_model_name == 'LogisticRegression':
    X_all_scaled = scaler.transform(X_all)
    all_proba = models[best_model_name].predict_proba(X_all_scaled)[:, 1]
elif best_model_name == 'Ensemble':
    all_proba = ensemble_model.predict_proba(X_all)[:, 1]
else:
    all_proba = models[best_model_name].predict_proba(X_all)[:, 1]

data['churn_probability'] = all_proba

# ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜
data['risk_level'] = pd.cut(data['churn_probability'], 
                            bins=[0, 0.3, 0.5, 0.7, 1.0],
                            labels=['Low', 'Medium', 'High', 'Very High'])

risk_summary = data.groupby('risk_level').agg({
    'CustomerID': 'count',
    'Monetary': ['mean', 'sum'],
    'Frequency': 'mean',
    'is_churned': 'mean',
    'churn_probability': 'mean'
}).round(2)

print("\nìœ„í—˜ ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬:")
print(risk_summary)

# ========================================================================
# 14. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
# ========================================================================
print("\nğŸ’¼ 14ë‹¨ê³„: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸")
print("-" * 50)

# 1. ê³ ìœ„í—˜ ê³ ê° ì‹ë³„
high_risk_threshold = 0.7
high_risk_customers = data[data['churn_probability'] >= high_risk_threshold].copy()
high_risk_customers = high_risk_customers.sort_values('churn_probability', ascending=False)

print(f"\nğŸ”´ ê³ ìœ„í—˜ ê³ ê° (ì´íƒˆ í™•ë¥  â‰¥ {high_risk_threshold:.0%}):")
print(f"  - ê³ ê° ìˆ˜: {len(high_risk_customers):,}ëª…")
print(f"  - í‰ê·  Frequency: {high_risk_customers['Frequency'].mean():.1f}")
print(f"  - í‰ê·  Monetary: Â£{high_risk_customers['Monetary'].mean():.2f}")
print(f"  - ì ì¬ ì†ì‹¤ ë§¤ì¶œ: Â£{high_risk_customers['Monetary'].sum():,.2f}")

# 2. ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  ì´íƒˆ í™•ë¥ 
segment_risk = data.groupby('Segment').agg({
    'churn_probability': ['mean', 'std'],
    'CustomerID': 'count',
    'Monetary': 'sum'
}).round(3)
segment_risk.columns = ['_'.join(col).strip() for col in segment_risk.columns]
segment_risk = segment_risk.sort_values('churn_probability_mean', ascending=False)

print("\nğŸ“Š ì„¸ê·¸ë¨¼íŠ¸ë³„ ì´íƒˆ ìœ„í—˜ë„:")
print(segment_risk.head(10))

# 3. ì¡°ê¸° ê²½ê³  ì‹ í˜¸ ê³ ê°
early_warning = data[
    (data['purchase_interval_trend'] > 20) & 
    (data['recent_activity_ratio_30d'] < 0.2) &
    (data['is_churned'] == 0) &
    (data['churn_probability'] >= 0.5)
]

print(f"\nâš ï¸ ì¡°ê¸° ê²½ê³  ì‹ í˜¸ ê³ ê°:")
print(f"  - ëŒ€ìƒ: {len(early_warning):,}ëª…")
print(f"  - í‰ê·  ì´íƒˆ í™•ë¥ : {early_warning['churn_probability'].mean():.1%}")
print(f"  - ìœ„í—˜ ë§¤ì¶œ: Â£{early_warning['Monetary'].sum():,.2f}")

# ========================================================================
# 15. ROI ê¸°ë°˜ ê°œì… ì „ëµ
# ========================================================================
print("\nğŸ’° 15ë‹¨ê³„: ROI ê¸°ë°˜ ê°œì… ì „ëµ")
print("-" * 50)

# ë„ë©”ì¸ ì§€ì‹: ì¬ì°¸ì—¬ ìº í˜ì¸ ë¹„ìš© ë° íš¨ê³¼ ê°€ì •
campaign_cost_per_customer = 5  # Â£5 per customer
reactivation_success_rate = 0.25  # 25% ì„±ê³µë¥ 
avg_reactivation_value = data[data['is_churned'] == 0]['avg_order_value'].mean()

print(f"\nìº í˜ì¸ ê°€ì •:")
print(f"  - ê³ ê°ë‹¹ ë¹„ìš©: Â£{campaign_cost_per_customer}")
print(f"  - ì˜ˆìƒ ì„±ê³µë¥ : {reactivation_success_rate:.0%}")
print(f"  - ì¬í™œì„±í™” í‰ê·  ê°€ì¹˜: Â£{avg_reactivation_value:.2f}")

for threshold in [0.5, 0.6, 0.7, 0.8]:
    target_customers = data[
        (data['churn_probability'] >= threshold) & 
        (data['is_churned'] == 0)
    ]
    
    if len(target_customers) > 0:
        expected_cost = len(target_customers) * campaign_cost_per_customer
        expected_reactivations = len(target_customers) * reactivation_success_rate
        expected_revenue = expected_reactivations * avg_reactivation_value
        expected_profit = expected_revenue - expected_cost
        expected_roi = (expected_profit / expected_cost * 100) if expected_cost > 0 else 0
        
        print(f"\nì„ê³„ê°’ {threshold:.0%}:")
        print(f"  - ëŒ€ìƒ ê³ ê°: {len(target_customers):,}ëª…")
        print(f"  - ì˜ˆìƒ ë¹„ìš©: Â£{expected_cost:,.0f}")
        print(f"  - ì˜ˆìƒ ì¬í™œì„±í™”: {expected_reactivations:.0f}ëª…")
        print(f"  - ì˜ˆìƒ ìˆ˜ìµ: Â£{expected_revenue:,.0f}")
        print(f"  - ì˜ˆìƒ ì´ìµ: Â£{expected_profit:,.0f}")
        print(f"  - ì˜ˆìƒ ROI: {expected_roi:.1f}%")

# ìµœì  ì„ê³„ê°’ ì°¾ê¸°
best_roi = -np.inf
best_threshold = 0.5

for threshold in np.arange(0.4, 0.9, 0.05):
    target_customers = data[
        (data['churn_probability'] >= threshold) & 
        (data['is_churned'] == 0)
    ]
    
    if len(target_customers) > 10:  # ìµœì†Œ 10ëª… ì´ìƒ
        expected_cost = len(target_customers) * campaign_cost_per_customer
        expected_reactivations = len(target_customers) * reactivation_success_rate
        expected_revenue = expected_reactivations * avg_reactivation_value
        expected_profit = expected_revenue - expected_cost
        roi = (expected_profit / expected_cost * 100) if expected_cost > 0 else 0
        
        if roi > best_roi:
            best_roi = roi
            best_threshold = threshold

print(f"\nğŸ¯ ìµœì  íƒ€ê²ŸíŒ… ì„ê³„ê°’: {best_threshold:.2f} (ì˜ˆìƒ ROI: {best_roi:.1f}%)")

# ========================================================================
# 16. ì‹œê°í™”
# ========================================================================
print("\nğŸ“Š 16ë‹¨ê³„: ì‹œê°í™” ìƒì„±")
print("-" * 50)

fig, axes = plt.subplots(3, 3, figsize=(20, 18))

# 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
ax = axes[0, 0]
models_list = list(all_scores.keys())
scores_list = list(all_scores.values())
colors = ['skyblue' if m != best_model_name else 'gold' for m in models_list]
bars = ax.bar(range(len(models_list)), scores_list, color=colors)
ax.set_xticks(range(len(models_list)))
ax.set_xticklabels(models_list, rotation=45, ha='right')
ax.set_ylabel('Validation AUC')
ax.set_title('Model Performance Comparison', fontweight='bold')
for bar, score in zip(bars, scores_list):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
           f'{score:.3f}', ha='center', va='bottom')
ax.set_ylim([min(scores_list) * 0.95, max(scores_list) * 1.05])

# 2. ROC ê³¡ì„ 
ax = axes[0, 1]
fpr, tpr, _ = roc_curve(y_test, test_proba)
ax.plot(fpr, tpr, label=f'{best_model_name} (AUC={test_auc:.3f})', linewidth=2, color='blue')
ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve - Test Set', fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Precision-Recall ê³¡ì„ 
ax = axes[0, 2]
precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, test_proba)
ax.plot(recall_vals, precision_vals, linewidth=2, color='green')
ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.set_title('Precision-Recall Curve', fontweight='bold')
ax.grid(True, alpha=0.3)
# F1 ìµœì ì  í‘œì‹œ
f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)
optimal_idx = np.argmax(f1_scores[:-1])
ax.scatter(recall_vals[optimal_idx], precision_vals[optimal_idx], 
          color='red', s=100, zorder=5, label=f'Best F1={f1_scores[optimal_idx]:.3f}')
ax.legend()

# 4. í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)
ax = axes[1, 0]
if 'feature_importance' in locals() and not feature_importance.empty:
    top_features = feature_importance.head(10)
    importance_col = 'Importance' if 'Importance' in top_features.columns else 'Avg_Importance'
    ax.barh(range(len(top_features)), top_features[importance_col].values, color='coral')
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features['Feature'].values)
    ax.set_xlabel('Importance')
    ax.set_title('Top 10 Feature Importance', fontweight='bold')
    ax.invert_yaxis()
else:
    ax.text(0.5, 0.5, 'Feature importance\nnot available', 
           ha='center', va='center', transform=ax.transAxes)
    ax.set_title('Feature Importance', fontweight='bold')

# 5. ì´íƒˆ í™•ë¥  ë¶„í¬
ax = axes[1, 1]
ax.hist(data[data['is_churned'] == 0]['churn_probability'], bins=30, 
        alpha=0.6, label='Active', color='green', edgecolor='black')
ax.hist(data[data['is_churned'] == 1]['churn_probability'], bins=30, 
        alpha=0.6, label='Churned', color='red', edgecolor='black')
ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Default Threshold')
ax.axvline(x=best_threshold, color='blue', linestyle='--', alpha=0.5, 
          label=f'Optimal ROI Threshold ({best_threshold:.2f})')
ax.set_xlabel('Churn Probability')
ax.set_ylabel('Number of Customers')
ax.set_title('Churn Probability Distribution', fontweight='bold')
ax.legend()

# 6. ìœ„í—˜ ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬
ax = axes[1, 2]
risk_counts = data['risk_level'].value_counts()
colors_risk = ['green', 'yellow', 'orange', 'red']
bars = ax.bar(range(len(risk_counts)), risk_counts.values, 
              color=[colors_risk[i] for i in range(len(risk_counts))])
ax.set_xticks(range(len(risk_counts)))
ax.set_xticklabels(risk_counts.index)
ax.set_ylabel('Number of Customers')
ax.set_title('Customer Distribution by Risk Level', fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{int(height)}\n({height/len(data)*100:.1f}%)',
           ha='center', va='bottom')

# 7. ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  ì´íƒˆ í™•ë¥ 
ax = axes[2, 0]
segment_churn_prob = data.groupby('Segment')['churn_probability'].mean().sort_values(ascending=False).head(10)
colors_seg = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(segment_churn_prob)))
bars = ax.bar(range(len(segment_churn_prob)), segment_churn_prob.values, color=colors_seg)
ax.set_xticks(range(len(segment_churn_prob)))
ax.set_xticklabels(segment_churn_prob.index, rotation=45, ha='right')
ax.set_ylabel('Average Churn Probability')
ax.set_title('Churn Risk by Segment', fontweight='bold')
ax.axhline(y=data['churn_probability'].mean(), color='black', linestyle='--', 
          alpha=0.5, label=f'Overall Avg: {data["churn_probability"].mean():.2f}')
ax.legend()

# 8. êµì°¨ ê²€ì¦ ê²°ê³¼
ax = axes[2, 1]
cv_data = pd.DataFrame({
    'Fold': [f'Fold {i+1}' for i in range(len(cv_scores))],
    'AUC': cv_scores
})
bars = ax.bar(cv_data['Fold'], cv_data['AUC'], color='lightblue', edgecolor='navy')
ax.axhline(y=cv_scores.mean(), color='red', linestyle='--', 
          label=f'Mean: {cv_scores.mean():.4f}')
ax.set_ylabel('AUC Score')
ax.set_title('Cross-Validation Results', fontweight='bold')
ax.set_ylim([cv_scores.min() * 0.95, cv_scores.max() * 1.02])
for bar, score in zip(bars, cv_scores):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
           f'{score:.3f}', ha='center', va='bottom')
ax.legend()

# 9. ROI ë¶„ì„
ax = axes[2, 2]
thresholds = np.arange(0.4, 0.9, 0.05)
rois = []
customer_counts = []

for threshold in thresholds:
    target = data[(data['churn_probability'] >= threshold) & (data['is_churned'] == 0)]
    if len(target) > 0:
        cost = len(target) * campaign_cost_per_customer
        revenue = len(target) * reactivation_success_rate * avg_reactivation_value
        roi = ((revenue - cost) / cost * 100) if cost > 0 else 0
        rois.append(roi)
        customer_counts.append(len(target))
    else:
        rois.append(0)
        customer_counts.append(0)

ax2 = ax.twinx()
line1 = ax.plot(thresholds, rois, 'b-', linewidth=2, label='ROI %')
bars = ax2.bar(thresholds, customer_counts, alpha=0.3, color='gray', width=0.03, label='Target Customers')
ax.set_xlabel('Probability Threshold')
ax.set_ylabel('ROI (%)', color='b')
ax2.set_ylabel('Number of Customers', color='gray')
ax.set_title('ROI Analysis by Threshold', fontweight='bold')
ax.axvline(x=best_threshold, color='red', linestyle='--', alpha=0.7, 
          label=f'Optimal: {best_threshold:.2f}')
ax.tick_params(axis='y', labelcolor='b')
ax2.tick_params(axis='y', labelcolor='gray')
ax.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.suptitle(f'Churn Prediction Model Analysis - {best_model_name}', 
            fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('churn_prediction_comprehensive.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: churn_prediction_comprehensive.png")

# ========================================================================
# 17. ê²°ê³¼ ì €ì¥
# ========================================================================
print("\nğŸ’¾ 17ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
print("-" * 50)

# 1. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
prediction_results = data[['CustomerID', 'Segment', 'Recency', 'Frequency', 'Monetary',
                          'is_churned', 'churn_probability', 'risk_level']].copy()
prediction_results = prediction_results.sort_values('churn_probability', ascending=False)
prediction_results.to_csv('churn_predictions_final.csv', index=False)
print("âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: churn_predictions_final.csv")

# 2. ê³ ìœ„í—˜ ê³ ê° ë¦¬ìŠ¤íŠ¸
high_risk_export = high_risk_customers[['CustomerID', 'Segment', 'churn_probability',
                                        'Recency', 'Frequency', 'Monetary',
                                        'risk_score', 'recent_activity_ratio_30d']].copy()
high_risk_export.to_csv('high_risk_customers_final.csv', index=False)
print(f"âœ… ê³ ìœ„í—˜ ê³ ê° ë¦¬ìŠ¤íŠ¸: high_risk_customers_final.csv ({len(high_risk_export)}ëª…)")

# 3. ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
performance_summary = pd.DataFrame({
    'Model': [best_model_name],
    'Validation_AUC': [best_score],
    'Test_AUC': [test_auc],
    'Test_Precision': [test_precision],
    'Test_Recall': [test_recall],
    'Test_F1': [test_f1],
    'CV_Mean_AUC': [cv_scores.mean()],
    'CV_Std_AUC': [cv_scores.std()],
    'Optimal_ROI_Threshold': [best_threshold],
    'Expected_ROI': [best_roi]
})
performance_summary.to_csv('model_performance_final.csv', index=False)
print("âœ… ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½: model_performance_final.csv")

# 4. í”¼ì²˜ ì¤‘ìš”ë„
if 'feature_importance' in locals() and not feature_importance.empty:
    feature_importance.to_csv('feature_importance_final.csv', index=False)
    print("âœ… í”¼ì²˜ ì¤‘ìš”ë„: feature_importance_final.csv")

# 5. ì„¸ê·¸ë¨¼íŠ¸ë³„ ìœ„í—˜ë„ ë¶„ì„
segment_analysis = data.groupby('Segment').agg({
    'CustomerID': 'count',
    'churn_probability': ['mean', 'std'],
    'is_churned': 'mean',
    'Monetary': ['mean', 'sum'],
    'risk_score': 'mean',
    'recent_activity_ratio_30d': 'mean'
}).round(3)
segment_analysis.columns = ['_'.join(col).strip() for col in segment_analysis.columns]
segment_analysis.to_csv('segment_risk_analysis_final.csv')
print("âœ… ì„¸ê·¸ë¨¼íŠ¸ ìœ„í—˜ë„ ë¶„ì„: segment_risk_analysis_final.csv")

# 6. ëª¨ë¸ ì €ì¥ (ì¬ì‚¬ìš©ì„ ìœ„í•´)
import pickle
model_package = {
    'model': final_model,
    'scaler': scaler if best_model_name == 'LogisticRegression' else None,
    'features': available_features,
    'model_name': best_model_name,
    'threshold': best_threshold,
    'performance': {
        'auc': test_auc,
        'precision': test_precision,
        'recall': test_recall,
        'f1': test_f1
    }
}
with open('churn_prediction_model.pkl', 'wb') as f:
    pickle.dump(model_package, f)
print("âœ… ëª¨ë¸ ì €ì¥: churn_prediction_model.pkl")

# ========================================================================
# 18. ìµœì¢… ìš”ì•½
# ========================================================================
print("\n" + "=" * 80)
print("ğŸ¯ í†µí•© ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
print("=" * 80)

print("\nğŸ“Š ë°ì´í„° ìš”ì•½:")
print(f"  - ì´ ê³ ê° ìˆ˜: {len(data):,}ëª…")
print(f"  - ì´íƒˆë¥ : {data['is_churned'].mean():.1%}")
print(f"  - ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(available_features)}")

print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥:")
print(f"  - ìµœì¢… ëª¨ë¸: {best_model_name}")
print(f"  - í…ŒìŠ¤íŠ¸ AUC: {test_auc:.4f}")
print(f"  - êµì°¨ ê²€ì¦ AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
print(f"  - Precision: {test_precision:.4f}")
print(f"  - Recall: {test_recall:.4f}")
print(f"  - F1-Score: {test_f1:.4f}")

print("\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
if 'feature_importance' in locals() and not feature_importance.empty:
    top3_features = feature_importance.head(3)
    print("  ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ë³€ìˆ˜:")
    for idx, row in top3_features.iterrows():
        importance_col = 'Importance' if 'Importance' in row else 'Avg_Importance'
        print(f"    {idx+1}. {row['Feature']}: {row[importance_col]:.4f}")

print("\nğŸš€ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:")
print(f"  - ê³ ìœ„í—˜ ê³ ê°: {len(high_risk_customers):,}ëª…")
print(f"  - ì ì¬ ë°©ì–´ ê°€ëŠ¥ ë§¤ì¶œ: Â£{high_risk_customers['Monetary'].sum():,.0f}")
print(f"  - ìµœì  íƒ€ê²ŸíŒ… ì„ê³„ê°’: {best_threshold:.2f}")
print(f"  - ì˜ˆìƒ ìº í˜ì¸ ROI: {best_roi:.1f}%")

print("\nğŸ“ˆ ê¶Œì¥ ë‹¤ìŒ ë‹¨ê³„:")
print("  1. ê³ ìœ„í—˜ ê³ ê° ëŒ€ìƒ ë§ì¶¤í˜• ì¬ì°¸ì—¬ ìº í˜ì¸ ì‹¤í–‰")
print("  2. A/B í…ŒìŠ¤íŠ¸ë¡œ ê°œì… ì „ëµ íš¨ê³¼ ê²€ì¦")
print("  3. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•")
print("  4. ì‹¤ì‹œê°„ ìŠ¤ì½”ì–´ë§ API ê°œë°œ")
print("  5. ì •ê¸°ì  ëª¨ë¸ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")

print("\nâœ¨ í†µí•© ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ ì™„ì„±!")
print("=" * 80)